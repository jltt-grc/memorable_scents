{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Data Cleaning and Preparation Notebook  \n",
    "\n",
    "## ðŸŽ¯ Objective  \n",
    "This notebook is designed to clean and preprocess the raw dataset for further analysis.\n",
    "It includes steps such as handling missing values, computing perceptual distances, and performing text preprocessing.\n",
    "\n",
    "## ðŸ“‘ Table of Contents  \n",
    "\n",
    "- [Libraries](#toc1_)  \n",
    "  *Importing necessary libraries for data manipulation, visualization, and NLP.*  \n",
    "- [Functions](#toc2_)  \n",
    "  *Defining functions used throughout the notebook.*  \n",
    "- [Data Preparation](#toc3_)  \n",
    "  - [Loading](#toc3_1_) â€“ *Load the dataset into a DataFrame.*  \n",
    "  - [Delete NA](#toc3_2_) â€“ *Remove missing values to ensure data consistency.*  \n",
    "  - [Compute Emotional Strength](#toc3_3_) â€“ *Derive the emotional strength metric based on pleasantness.*  \n",
    "  - [Add Memory Column](#toc3_4_) â€“ *Create a new variable to analyze associative memory.*  \n",
    "  - [Encode Gender](#toc3_5_) â€“ *Convert gender information into a numerical format.*  \n",
    "  - [Subset DataFrame](#toc3_6_) â€“ *Filter the dataset to retain relevant observations.*  \n",
    "- [Perceptual Distance](#toc4_)  \n",
    "  - [Participant Perceptual Space](#toc4_1_) â€“ *Visualize participants' perceptual space.*  \n",
    "  - [Compute Euclidean Distances](#toc4_2_) â€“ *Measure the perceptual similarity between observations.*  \n",
    "- [NLP Processing](#toc5_)  \n",
    "  - [Text Cleaning](#toc5_1_) â€“ *Remove special characters, extra spaces, and unwanted symbols.*  \n",
    "  - [Normalization](#toc5_2_) â€“ *Standardize text formatting (e.g., lowercasing).*  \n",
    "  - [Tokenization](#toc5_3_) â€“ *Split text into individual words.*  \n",
    "  - [Lemmatization](#toc5_4_) â€“ *Reduce words to their base form.*  \n",
    "  - [Word Count](#toc5_5_) â€“ *Analyze word frequencies.*  \n",
    "  - [Compute Jaccard Distance](#toc5_6_) â€“ *Measure similarity between text samples using Jaccard distance.*  \n",
    "- [Save Processed Data](#toc6_)  \n",
    "  *Export the cleaned and transformed dataset for further use.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import art3d\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "from spacy.language import Language\n",
    "from spacy.lookups import Lookups\n",
    "\n",
    "from exceptions_lemma import lemma_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_convex_hulls(df, participants, features, show_volumes=False):\n",
    "    def plot_convex_hull(ax, points, color, show_volumes):\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            for simplex in hull.simplices:\n",
    "                ax.plot(points[simplex, 0], points[simplex, 1], points[simplex, 2], color=color)\n",
    "            if show_volumes:\n",
    "                for simplex in hull.simplices:\n",
    "                    triangle = points[simplex]\n",
    "                    poly = art3d.Poly3DCollection([triangle], color=color, alpha=0.3)\n",
    "                    ax.add_collection3d(poly)\n",
    "            return hull\n",
    "        except QhullError:\n",
    "            print(f\"Could not compute convex hull for points\")\n",
    "            return None\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    colors = [\"#FF6F61\", \"#6B5B95\", \"#88B04B\"]\n",
    "    \n",
    "    # Dictionary to rename participants\n",
    "    participant_labels = {\n",
    "        'AB': 'Participant 1',\n",
    "        'AL': 'Participant 2',\n",
    "        'AMB': 'Participant 3'\n",
    "        }\n",
    "\n",
    "    for i, participant in enumerate(participants):\n",
    "        participant_points = df[df['participant'] == participant][features].values\n",
    "        label = participant_labels.get(participant, f'Participant {i + 1}')\n",
    "        ax.scatter(\n",
    "            participant_points[:, 0],\n",
    "            participant_points[:, 1],\n",
    "            participant_points[:, 2],\n",
    "            label=label,\n",
    "            color=colors[i],\n",
    "            s=100,\n",
    "            )\n",
    "        plot_convex_hull(ax, participant_points, colors[i], show_volumes)\n",
    "\n",
    "    # Adjust axis limits\n",
    "    ## Pleasantness\n",
    "    ax.set_xlim([-5.2, 5.2])\n",
    "    ax.set_xticks(np.arange(-5, 6, 2.5))\n",
    "    ## Familiarity\n",
    "    ax.set_ylim([-0.2, 10.2])\n",
    "    ax.set_yticks(np.arange(0, 11, 2.5))\n",
    "    ## Intensity\n",
    "    ax.set_zlim([-0.2, 10.2])\n",
    "    ax.set_zticks(np.arange(0, 11, 2.5))\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Pleasantness', fontsize=18, labelpad=20)\n",
    "    ax.set_ylabel('Familiarity', fontsize=18, labelpad=20)\n",
    "    ax.set_zlabel('Intensity', fontsize=18, labelpad=15)\n",
    "\n",
    "    # Set tick label font size\n",
    "    ax.tick_params(axis='x', labelsize=18, length=10, width=4)\n",
    "    ax.tick_params(axis='y', labelsize=18, length=10, width=4)\n",
    "    ax.tick_params(axis='z', labelsize=18, length=10, width=4, pad=10)\n",
    "\n",
    "    # Set axis spines thickness\n",
    "    ax.xaxis.line.set_linewidth(2)\n",
    "    ax.yaxis.line.set_linewidth(2)\n",
    "    ax.zaxis.line.set_linewidth(2)\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(\n",
    "        fontsize=18,\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1.1, 0.5)\n",
    "    )\n",
    "\n",
    "    #ax.view_init(elev=10, azim=300)\n",
    "    ax.view_init(elev=10, azim=310)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_distance(df, features):\n",
    "    results = []\n",
    "\n",
    "    for participant, group in df.groupby('participant'):\n",
    "        odors = group[['odor_name'] + features].reset_index(drop=True)\n",
    "\n",
    "        # Iterate on all scent combinations\n",
    "        for i in range(len(odors)):\n",
    "            for j in range(len(odors)):\n",
    "                if i != j:\n",
    "                    odor_i = odors.iloc[i]\n",
    "                    odor_j = odors.iloc[j]\n",
    "                    distance = euclidean(\n",
    "                        [odor_i[feature] for feature in features],\n",
    "                        [odor_j[feature] for feature in features]\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'participant': participant,\n",
    "                        'odor_name': odor_i['odor_name'],\n",
    "                        'odor_name_2': odor_j['odor_name'],\n",
    "                        'euclidean_distance': distance\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute jaccard distance\n",
    "def jaccard_distance(set1, set2):\n",
    "    if not set1 and not set2:\n",
    "        return 0.0  # Both sets are empty, so they are identical\n",
    "    if not set1 or not set2:\n",
    "        return 1.0  # One of the sets is empty, so they are completely disjoint\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return 1 - len(intersection) / len(union)\n",
    "\n",
    "# To compute pairwsise jaccard distance btw odors\n",
    "def compute_jaccard_distance(df, participant_column='participant', odor_name_column='odor_name', descriptors_column='lemma'):\n",
    "    results = []\n",
    "\n",
    "    # Group by participant\n",
    "    grouped = df.groupby(participant_column)\n",
    "\n",
    "    for participant, group in grouped:\n",
    "        # Extract odor_name and descriptors\n",
    "        odor_name = group[odor_name_column].values\n",
    "        descriptors = group[descriptors_column].apply(lambda x: set(x) if isinstance(x, list) else set()).values\n",
    "        \n",
    "        # Calculate Jaccard distance for each pair of lines\n",
    "        num_descriptors = len(descriptors)\n",
    "        for i in range(num_descriptors):\n",
    "            for j in range(num_descriptors):\n",
    "                if i != j:\n",
    "                    jaccard_dist = jaccard_distance(descriptors[i], descriptors[j])\n",
    "                    results.append({\n",
    "                        'participant': participant,\n",
    "                        'odor_name': odor_name[i],\n",
    "                        'odor_name2': odor_name[j],\n",
    "                        'descriptors_lem': descriptors[i],\n",
    "                        'descriptors_lem2': descriptors[j],\n",
    "                        'jaccard_distance': jaccard_dist\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data prep](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Loading](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory of the current working directory\n",
    "project_dir = Path.cwd().parent\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_folder = project_dir / \"data\"\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = data_folder / \"raw_data.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Delete NA](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete lines where pleasantness, intensity or familiarity contain NaN\n",
    "df = df.dropna(subset=['pleasantness', 'intensity', 'familiarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Compute emotional strength](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(df.columns.get_loc('pleasantness') + 1, 'emotional_strength', df['pleasantness'].abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[Add mem col](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column 'mem' to indicate if the odor was associated with a memory\n",
    "df.insert(df.columns.get_loc('what') + 1, 'mem', df.apply(lambda row: 1 if row['www'] == 1 or row['wwhich'] == 1 else 0, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Encode gender](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode W=0, M=1\n",
    "df.insert(df.columns.get_loc('gender') + 1, 'gender_encoded', df['gender'].map({'W': 0, 'M': 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_6_'></a>[Subset df](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset df\n",
    "df_target = df[df['is_target'] == 1]\n",
    "df_hit = df[df['hit'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Perceptual distance](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Participant perceptual space](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features selection\n",
    "features = ['pleasantness', 'familiarity', 'intensity']\n",
    "\n",
    "# Participants selection\n",
    "participants = ['BT', 'PAC', 'DNTT']\n",
    "\n",
    "# Filter data for selected participants\n",
    "df_target_filtered = df_target[df_target['participant'].isin(participants)]\n",
    "\n",
    "# Function\n",
    "plot_3d_convex_hulls(df_target_filtered, participants, features, show_volumes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features selection\n",
    "features = ['pleasantness', 'familiarity', 'intensity']\n",
    "\n",
    "# Participants selection\n",
    "participants = ['BT', 'PAC', 'DNTT']\n",
    "\n",
    "# Filter data for selected participants\n",
    "df_hit_filtered = df_hit[df_hit['participant'].isin(participants)]\n",
    "\n",
    "# Function\n",
    "plot_3d_convex_hulls(df_hit_filtered, participants, features, show_volumes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Compute euclidean distances](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic definition\n",
    "features = ['pleasantness', 'intensity', 'familiarity']\n",
    "\n",
    "# Calculating distances for each DataFrame\n",
    "euclidean_target = compute_euclidean_distance(df_target, features)\n",
    "euclidean_hit = compute_euclidean_distance(df_hit, features)\n",
    "\n",
    "# Average Euclidean distances per scent and participant\n",
    "mean_euclidean_target = euclidean_target.groupby(['participant', 'odor_name'])[['euclidean_distance']].mean().reset_index()\n",
    "mean_euclidean_hit = euclidean_hit.groupby(['participant', 'odor_name'])[['euclidean_distance']].mean().reset_index()\n",
    "\n",
    "# Rename col\n",
    "mean_euclidean_target = mean_euclidean_target.rename(columns={'euclidean_distance': 'avg_distance_target'})\n",
    "mean_euclidean_hit = mean_euclidean_hit.rename(columns={'euclidean_distance': 'avg_distance_hit'})\n",
    "\n",
    "# Merge with df\n",
    "df = pd.merge(df, mean_euclidean_target, on=['participant', 'odor_name'], how='outer')\n",
    "df = pd.merge(df, mean_euclidean_hit, on=['participant', 'odor_name'], how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[NLP](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Text cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ponctuation and special characters\n",
    "df['descriptors_clean'] = df['descriptors'].str.replace(r'[^\\w\\sÃ€-Ã¿]|_', ' ', regex=True)\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words_fr = set(stopwords.words('french'))\n",
    "\n",
    "# Adding words to stop words\n",
    "to_add = {\n",
    "    'Ã§a', 'a', 'sd', 'Ã l', 'x', 'pr',\n",
    "    'odeur', 'odeurs',\n",
    "    'sent', 'sens', 'sentie',\n",
    "    }\n",
    "stop_words_fr.update(to_add)\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        return ' '.join([word for word in text.split() if word.lower() not in stop_words_fr])\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Remove stop words\n",
    "df['descriptors_sw'] = df['descriptors_clean'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Normalization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase conversion\n",
    "df['descriptors_lc'] = df['descriptors_sw'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Tokenization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Function to tokenize the col\n",
    "def spacy_tokenizer(text):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        return [token.text for token in doc]\n",
    "\n",
    "# Apply to DF\n",
    "df['tokens'] = df['descriptors_lc'].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Lemmatization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lookups\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_lookup\", lemma_lookup)\n",
    "\n",
    "# Define a spaCy component to use the lemmatization table\n",
    "@Language.component(\"lemma_correction_component\")\n",
    "def lemma_correction_component(doc):\n",
    "    lemma_table = lookups.get_table(\"lemma_lookup\")\n",
    "    for token in doc:\n",
    "        if token.text in lemma_table:\n",
    "            token.lemma_ = lemma_table[token.text]\n",
    "    return doc\n",
    "\n",
    "# Add component to spaCy pipeline\n",
    "nlp.add_pipe(\"lemma_correction_component\", after=\"lemmatizer\")\n",
    "\n",
    "# Function for lemmatizing a list of tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    if not isinstance(tokens, list) or not all(isinstance(token, str) for token in tokens):\n",
    "        return []                          # Return an empty list if tokens is not a string list\n",
    "    if not tokens:\n",
    "        return tokens\n",
    "    text = \" \".join(tokens)                # Join tokens into a chain and analyze with spaCy\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc] # Extract token lemmas\n",
    "    \n",
    "# Lemmatize\n",
    "df['lemma'] = df['tokens'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_5_'></a>[Word count](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count words\n",
    "def count_words(tokens):\n",
    "    if isinstance(tokens, (list, np.ndarray)):\n",
    "        return len(tokens)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Word count\n",
    "df['nb_words'] = df['lemma'].apply(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_6_'></a>[Compute Jaccard Distance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset df\n",
    "df_target = df[df['is_target'] == 1]\n",
    "df_hit = df[df['hit'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise jaccard distance\n",
    "jaccard_target = compute_jaccard_distance(df_target, descriptors_column='lemma')\n",
    "jaccard_hit = compute_jaccard_distance(df_hit, descriptors_column='lemma')\n",
    "\n",
    "# Mean jaccard dist by_odor by_subj\n",
    "mean_jaccard_target = jaccard_target.groupby(['participant', 'odor_name'])[['jaccard_distance']].mean()\n",
    "mean_jaccard_hit = jaccard_hit.groupby(['participant', 'odor_name'])[['jaccard_distance']].mean()\n",
    "\n",
    "# Rename col\n",
    "mean_jaccard_target = mean_jaccard_target.rename(columns={'jaccard_distance': 'mean_lemma_jaccard_target'})\n",
    "mean_jaccard_hit = mean_jaccard_hit.rename(columns={'jaccard_distance': 'mean_lemma_jaccard_hit'})\n",
    "\n",
    "# Merge with df\n",
    "df = pd.merge(df, mean_jaccard_target, on=['participant', 'odor_name'], how='outer')\n",
    "df = pd.merge(df, mean_jaccard_hit, on=['participant', 'odor_name'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Save](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select col to save (to heavy w/ all col)\n",
    "col_selection = [\n",
    "    \"study\", \"participant\",\"gender\", \"gender_encoded\",\n",
    "    \"pres_order\", \"odor_num\", \"odor_name\", \"is_target\", \"day\",\n",
    "    \"hit\", \"cr\", \"www\", \"wwhich\", \"what\", \"mem\",\n",
    "    \"pleasantness\", \"emotional_strength\", \"intensity\", \"familiarity\",\n",
    "    \"avg_distance_target\", \"avg_distance_hit\",\n",
    "    \"descriptors\",\n",
    "    \"tokens\", \"lemma\",\n",
    "    \"nb_words\",\n",
    "    \"mean_lemma_jaccard_target\", \"mean_lemma_jaccard_hit\"\n",
    "    ]\n",
    "\n",
    "df_sel = df[col_selection]\n",
    "\n",
    "# Save df\n",
    "csv_path = data_folder / 'dataset.csv'\n",
    "df_sel.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_oldmem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
