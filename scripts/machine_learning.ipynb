{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Machine Learning Analysis on Processed Data  \n",
    "\n",
    "## ðŸŽ¯ Objective  \n",
    "This notebook performs machine learning analysis on the processed dataset.  \n",
    "It includes data preprocessing specific to machine learning (feature/target separation, train/test splitting, and data shuffling), feature selection, model training, and evaluation.  \n",
    "\n",
    "The analysis can focus on one of two conditions:  \n",
    "- **Odor Recognition**  \n",
    "- **Associative Memory**  \n",
    "\n",
    "ðŸ“Œ **Important:** The condition selection is done in the step **[Condition Selection](#toc3_2_)**, where the user must specify `\"odor_recognition\"` or `\"associative_memory\"` before proceeding with the analysis.  \n",
    "\n",
    "## ðŸ“‘ Table of Contents  \n",
    "\n",
    "- [Libraries](#toc1_)  \n",
    "  *Importing necessary libraries for machine learning and data analysis.*  \n",
    "- [Functions](#toc2_)  \n",
    "  *Defining functions used throughout the notebook.*  \n",
    "- [Data Loading & Preprocessing](#toc3_)  \n",
    "  - [Loading](#toc3_1_) â€“ *Load the processed dataset into a DataFrame.*  \n",
    "  - [Condition Selection](#toc3_2_) â€“ *Select the analysis condition: `\"odor_recognition\"` or `\"associative_memory\"`.*  \n",
    "  - [Info on Data](#toc3_3_)  \n",
    "    - [Check for Unique Values](#toc3_3_1_) â€“ *Inspect key dataset attributes (e.g., number of participants, number of odors, etc.).*  \n",
    "    - [Target Distribution](#toc3_3_2_) â€“ *Visualize the distribution of the target variable.*  \n",
    "  - [Features/Target Split](#toc3_4_) â€“ *Separate predictor variables and target variable.*  \n",
    "  - [Train/Test Split](#toc3_5_) â€“ *Split data into training and testing sets.*  \n",
    "  - [Data Shuffling](#toc3_6_) â€“ *Randomize data order to prevent biases.*  \n",
    "- [Feature Selection](#toc4_)  \n",
    "  *Select the most relevant features for model training.*  \n",
    "- [Base Model](#toc5_)  \n",
    "  *Train and evaluate a baseline model without hyperparameter tuning.*  \n",
    "- [All Features - Models with Tuning](#toc6_)  \n",
    "  *Train machine learning models using all features with hyperparameter tuning.*  \n",
    "- [Final Models](#toc7_)  \n",
    "  *Train the final models using selected features and optimized hyperparameters.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedGroupKFold,\n",
    "    cross_validate,\n",
    "    permutation_test_score,\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_mapping = {\n",
    "    \"gender_encoded\": \"Gender\",\n",
    "    \"pleasantness\": \"Pleasantness\",\n",
    "    \"emotional_strength\": \"Emotional strength\",\n",
    "    \"intensity\": \"Intensity\",\n",
    "    \"familiarity\": \"Familiarity\",\n",
    "    \"percept_dist\": \"Perceptive distance\",\n",
    "    \"nb_words\": \"Number of words\",\n",
    "    \"jaccard_dist\": \"Semantic distance\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_random_forest(X_train, y_train, groups_train, model_type=\"default_rf\", tuning=False, categorical_features=\"No\"):\n",
    "    # Define categorical feature\n",
    "    if categorical_features == \"Yes\":\n",
    "        categorical_columns = [\"gender_encoded\"]\n",
    "        cat_col_index = [X_train.columns.get_loc(col) for col in categorical_columns if col in X_train.columns]\n",
    "    else:\n",
    "        cat_col_index = None\n",
    "\n",
    "    # Define classifier\n",
    "    if model_type == \"default_rf\":\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "    elif model_type == \"balanced_rf\":\n",
    "        model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "    elif model_type == \"smote_rf\":\n",
    "        if categorical_features == \"Yes\":\n",
    "            smote = SMOTENC(categorical_features=cat_col_index, random_state=42)\n",
    "        else:\n",
    "            smote = SMOTE(random_state=42)\n",
    "    \n",
    "        model = imbpipeline([\n",
    "            ('smote', smote),\n",
    "            ('rfc', RandomForestClassifier(random_state=42))\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose from 'default_rf', 'balanced_rf', 'smote_rf'.\")\n",
    "    \n",
    "    # Define cross-validation\n",
    "    cv = StratifiedGroupKFold(n_splits=10)\n",
    "    scoring = [\"accuracy\", \"roc_auc\", \"balanced_accuracy\", \"recall\", \"precision\", \"f1\", \"f1_weighted\"]\n",
    "    \n",
    "    # If no tuning, perform cross-validation\n",
    "    if not tuning:\n",
    "        cv_results = cross_validate(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            groups=groups_train,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True\n",
    "            )\n",
    "        \n",
    "        print(\"Average Performance Scores:\")\n",
    "        for metric in scoring:\n",
    "            train_score = np.mean(cv_results[f\"train_{metric}\"])\n",
    "            test_score = np.mean(cv_results[f\"test_{metric}\"])\n",
    "            print(f\"{metric}: Train - {train_score}, Validation - {test_score}\")\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        return model, cv_results\n",
    "    \n",
    "    # Define the parameters to be tested with BayesSearchCV\n",
    "    param_distributions = {\n",
    "        'n_estimators': Integer(500, 2000),\n",
    "        'max_depth': Integer(3, 50, prior='uniform'),\n",
    "        'max_leaf_nodes': Integer(10, 300, prior='uniform'),\n",
    "        'min_samples_leaf': Integer(1, 20),\n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None])\n",
    "    }\n",
    "\n",
    "    # If SMOTE is used, prepend 'rfc__' to parameters\n",
    "    if model_type == \"smote_rf\":\n",
    "        param_distributions = {f'rfc__{key}': value for key, value in param_distributions.items()}\n",
    "\n",
    "    # If tuning, search for the best hyperparameters with BayesSearchCV\n",
    "    search_cv = BayesSearchCV(\n",
    "        model,\n",
    "        param_distributions,\n",
    "        n_iter=100,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        refit=\"roc_auc\",\n",
    "        random_state=42\n",
    "        )\n",
    "    search_cv.fit(X_train, y_train, groups=groups_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = search_cv.best_estimator_\n",
    "    print(f\"Best model: {best_model}\")\n",
    "    \n",
    "    # Results\n",
    "    columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "    columns += [f\"mean_test_{metric}\" for metric in scoring]\n",
    "    cv_results_df = pd.DataFrame(search_cv.cv_results_)\n",
    "\n",
    "    return best_model, cv_results_df[columns].sort_values(by=\"mean_test_roc_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_xgboost(X_train, y_train, groups_train):\n",
    "    # Calculation of scale_pos_weight based on class balance\n",
    "    scale_pos_weight = y.value_counts(normalize=True)[0] / y.value_counts(normalize=True)[1]  # class_0 / class_1\n",
    "\n",
    "    # XGBoost classifier initialization\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        random_state=42,\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "\n",
    "    # Define the parameters to be tested with BayesSearchCV\n",
    "    param_distributions = {\n",
    "        'n_estimators': Integer(500, 2000),\n",
    "        'max_depth': Integer(3, 50, prior='uniform'),\n",
    "        'max_leaves': Integer(10, 300, prior='uniform'),\n",
    "        'min_child_weight': Integer(1, 20),\n",
    "        'gamma': Real(0, 10),\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree': Categorical([0.25, 0.5, 1.0]),\n",
    "        'num_parallel_tree': Integer(1, 10)\n",
    "    }\n",
    "\n",
    "    # Define cross-validation\n",
    "    cv = StratifiedGroupKFold(n_splits=10)\n",
    "    scoring = [\"accuracy\", \"roc_auc\", \"balanced_accuracy\", \"recall\", \"precision\", \"f1\", \"f1_weighted\"]\n",
    "\n",
    "   # Searching for the best hyperparameters with BayesSearchCV\n",
    "    search_cv = BayesSearchCV(\n",
    "        model,\n",
    "        param_distributions,\n",
    "        n_iter=100,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        refit=\"roc_auc\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    search_cv.fit(X_train, y_train, groups=groups_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = search_cv.best_estimator_\n",
    "    print(f\"Best model: {best_model}\")\n",
    "    \n",
    "    # Results\n",
    "    columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "    columns += [f\"mean_test_{metric}\" for metric in scoring]\n",
    "    cv_results_df = pd.DataFrame(search_cv.cv_results_)\n",
    "\n",
    "    return best_model, cv_results_df[columns].sort_values(by=\"mean_test_roc_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test_with_plot(model, X, y, groups, n_permutations=100, scoring=\"roc_auc\"):\n",
    "    # Define cv\n",
    "    cv = StratifiedGroupKFold(n_splits=10)\n",
    "    \n",
    "    # Perform permutation test\n",
    "    score, perm_scores, p_value = permutation_test_score(\n",
    "        model,\n",
    "        X, y, groups=groups,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_permutations=n_permutations,\n",
    "        n_jobs=-1,\n",
    "        random_state=42 \n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nPermutation test results:\\nTraining {scoring}: {score:.3f}, P-value: {p_value:.3f}\")\n",
    "\n",
    "    # Plot the distribution of permutation scores\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(perm_scores, bins=50, density=True, alpha=0.5, color='blue', label='Permutation Scores')\n",
    "    plt.axvline(score, color='red', linestyle='dashed', linewidth=2, label='Actual Score')\n",
    "\n",
    "    plt.xlabel(\"ROC AUC\", fontsize=20)\n",
    "    plt.ylabel(\"Density\", fontsize=20)\n",
    "    plt.legend(fontsize=16)\n",
    "\n",
    "    # plt.xlim(0.4, 0.61)\n",
    "    # plt.xticks([0.4, 0.45, 0.5, 0.55, 0.6], fontsize=20)\n",
    "    # plt.ylim(0, 33)\n",
    "    plt.grid(False)\n",
    "    sns.despine()\n",
    "\n",
    "    plt.gca().spines['bottom'].set_linewidth(2)\n",
    "    plt.gca().spines['left'].set_linewidth(2)\n",
    "    plt.tick_params(axis='both', length=10, labelsize=20, width=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return score, perm_scores, p_value, plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(best_model, X_test, y_test):\n",
    "    # Recovering the best model from the pipeline (if SMOTE/SMOTE-NC was used)\n",
    "    if hasattr(best_model, 'steps'):\n",
    "        best_model = best_model.steps[-1][1]\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion matrix:\",\n",
    "    pd.crosstab(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    rownames=['Actual class'],\n",
    "    colnames=['\\nPredicted class']\n",
    "    )\n",
    "    )\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Probabilities (for AUC calculation)\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
    "\n",
    "    # AUC score\n",
    "    auc_score = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"Test AUC: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_feature_selection(best_model, X_train, y_train, model=\"rf\"):\n",
    "    # Recovering the best model from the pipeline (if SMOTE/SMOTE-NC was used)\n",
    "    if hasattr(best_model, 'steps'):\n",
    "        best_model = best_model.steps[-1][1]\n",
    "\n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "    # Calculate SHAP values for train set\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "    # Map feature names\n",
    "    mapped_feature_names = [feature_name_mapping.get(name, name) for name in X_train.columns]\n",
    "    \n",
    "    # X_train with new names\n",
    "    X_train_mapped = X_train.copy()\n",
    "    X_train_mapped.columns = mapped_feature_names\n",
    "\n",
    "    # Bar plot of SHAP values\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bar_summary_plot = shap.summary_plot(shap_values, X_train_mapped, plot_type=\"bar\")\n",
    "\n",
    "    # Dot plot of SHAP values\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # If the model is a RF, dot plot of SHAP values for class 1\n",
    "    if model == \"rf\":\n",
    "        dot_summary_plot = shap.summary_plot(shap_values[1], X_train_mapped)\n",
    "    # If the model is a XGB, dot plot of SHAP values\n",
    "    elif model == \"xgb\":\n",
    "        dot_summary_plot = shap.summary_plot(shap_values, X_train_mapped)\n",
    "\n",
    "    return bar_summary_plot, dot_summary_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(best_model, X_test, y_test):\n",
    "    # Recovering the best model from the pipeline (if SMOTE/SMOTE-NC was used)\n",
    "    if hasattr(best_model, 'steps'):\n",
    "        best_model = best_model.steps[-1][1]\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Classification results\n",
    "    correct_class_0 = (y_test == 0) & (y_pred == 0)\n",
    "    correct_class_1 = (y_test == 1) & (y_pred == 1)\n",
    "    misclassified_class_0_as_1 = (y_test == 0) & (y_pred == 1)\n",
    "    misclassified_class_1_as_0 = (y_test == 1) & (y_pred == 0)\n",
    "\n",
    "    # Create DataFrame with classifications\n",
    "    df_errors = pd.concat([\n",
    "        X_test[correct_class_0].assign(classification='Correct Class 0'),\n",
    "        X_test[correct_class_1].assign(classification='Correct Class 1'),\n",
    "        X_test[misclassified_class_0_as_1].assign(classification='Misclassified 0->1'),\n",
    "        X_test[misclassified_class_1_as_0].assign(classification='Misclassified 1->0')\n",
    "    ], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Classification order\n",
    "    classification_order = ['Correct Class 0', 'Misclassified 1->0', 'Correct Class 1', 'Misclassified 0->1']\n",
    "\n",
    "    # Axes configuration\n",
    "    feature_config = {\n",
    "        \"Gender\": {\"xticks\": [0, 1], \"xticklabels\": ['W', 'M'], \"xlim\": (-0.5, 1.5)},\n",
    "        \"Pleasantness\": {\"xlim\": (-5, 5), \"xticks\": np.arange(-5, 6, 2.5)},\n",
    "        \"Emotional strength\": {\"xlim\": (0, 5), \"xticks\": np.arange(0, 6, 1)},\n",
    "        \"Intensity\": {\"xlim\": (0, 10), \"xticks\": np.arange(0, 11, 2.5)},\n",
    "        \"Familiarity\": {\"xlim\": (0, 10), \"xticks\": np.arange(0, 11, 2.5)},\n",
    "        \"Perceptive distance\": {\"xlim\": (0, 12), \"xticks\": np.arange(0, 13, 2)},\n",
    "        \"Number of words\": {\"xlim\": (0, 12), \"xticks\": np.arange(0, 13, 1), \"xticklabels\": np.arange(0, 13, 1)},\n",
    "        \"Semantic distance\": {\"xlim\": (0, 1), \"xticks\": np.arange(0, 1.25, 0.25)},\n",
    "    }\n",
    "\n",
    "    # Figure setup\n",
    "    num_features = len(X_test.columns)\n",
    "    rows = (num_features // 3) + 1\n",
    "    fig, axes = plt.subplots(rows, 3, figsize=(15, rows * 3.5))\n",
    "    axes = axes.flatten()\n",
    "    fontsize=16\n",
    "    labelsize=16\n",
    "\n",
    "    for i, feature_name in enumerate(X_test.columns):\n",
    "        ax = axes[i]\n",
    "        x_label = feature_name_mapping.get(feature_name, feature_name)\n",
    "\n",
    "        if feature_name == 'gender_encoded':\n",
    "            gender_counts = df_errors.groupby(['classification', 'gender_encoded']).size().unstack(fill_value=0)\n",
    "            gender_proportions = gender_counts.div(gender_counts.sum(axis=1), axis=0).reindex(classification_order)\n",
    "            gender_proportions = gender_proportions.rename(columns={0: 'W', 1: 'M'})\n",
    "\n",
    "            ax.barh(range(len(classification_order)), gender_proportions['W'], \n",
    "                     color=[palette[c] for c in classification_order], height=0.8, label='W')\n",
    "            ax.barh(range(len(classification_order)), gender_proportions['M'], \n",
    "                     left=gender_proportions['W'],\n",
    "                     color=[palette[c] for c in classification_order], height=0.8, label='M', hatch='//')\n",
    "\n",
    "            ax.set_xlim(-0.02, 1.02)\n",
    "            ax.set_xticks(np.arange(0, 1.25, 0.25))\n",
    "            ax.set_xlabel('Proportion', fontsize=fontsize)\n",
    "            ax.set_yticks(range(len(classification_order)))\n",
    "            ax.set_yticklabels(classification_order, fontsize=fontsize)\n",
    "            ax.set_ylabel('')\n",
    "            ax.yaxis.set_ticks_position('none')\n",
    "            ax.legend(title='Gender', loc='upper right', bbox_to_anchor=(1.5, 0.9), fontsize=fontsize, title_fontsize=fontsize)\n",
    "            ax.invert_yaxis()\n",
    "\n",
    "        else:\n",
    "            sns.stripplot(\n",
    "                data=df_errors,\n",
    "                x=feature_name,\n",
    "                y='classification',\n",
    "                order=classification_order,\n",
    "                hue='classification',\n",
    "                hue_order=classification_order,\n",
    "                dodge=False,\n",
    "                jitter=0.2,\n",
    "                palette=palette,\n",
    "                marker='o',\n",
    "                size=6,\n",
    "                ax=ax\n",
    "            )\n",
    "\n",
    "            # Apply feature-specific X-axis configuration\n",
    "            config = feature_config.get(x_label, {})\n",
    "            if \"xlim\" in config:\n",
    "                # Check if the feature is \"Semantic distance\"\n",
    "                if feature_name == \"jaccard_dist\":\n",
    "                    ax.set_xlim(config[\"xlim\"][0] - 0.02, config[\"xlim\"][1] + 0.02)\n",
    "                else:\n",
    "                    ax.set_xlim(config[\"xlim\"][0] - 0.2, config[\"xlim\"][1] + 0.2)\n",
    "\n",
    "            if \"xticks\" in config:\n",
    "                ax.set_xticks(config[\"xticks\"])\n",
    "\n",
    "            if \"xticklabels\" in config:\n",
    "                ax.set_xticklabels(config[\"xticklabels\"], fontsize=fontsize)\n",
    "\n",
    "            # Special case for \"Number of words\"\n",
    "            if feature_name == \"nb_words\":\n",
    "                ax.set_xlim(-0.2, 12.2)\n",
    "                ax.set_xticks(np.arange(0, 13, 1))\n",
    "                ax.set_xticklabels([i if i % 2 == 0 else '' for i in np.arange(0, 13, 1)], fontsize=fontsize)\n",
    "\n",
    "            ax.set_xlabel(x_label, fontsize=fontsize)\n",
    "            ax.set_ylabel('')\n",
    "            ax.yaxis.set_ticks_position('none')\n",
    "\n",
    "        # Display y labels only in first column\n",
    "        if i % 3 == 0:\n",
    "            ax.set_yticklabels(classification_order, fontsize=fontsize)\n",
    "        else:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        ax.tick_params(axis='both', length=10, width=2, labelsize=labelsize)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_linewidth(2)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "    # t-SNE or scatterplot depending on the number of features\n",
    "    if num_features > 2:\n",
    "        tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(df_errors[X_test.columns])\n",
    "        tsne_df = pd.DataFrame(X_tsne, columns=['t-SNE 1', 't-SNE 2'])\n",
    "        tsne_df['classification'] = df_errors['classification'].reset_index(drop=True)\n",
    "\n",
    "        ax_tsne = axes[num_features] if num_features < len(axes) else fig.add_subplot(rows, 3, num_features + 1)\n",
    "        sns.scatterplot(\n",
    "            data=tsne_df,\n",
    "            x='t-SNE 1',\n",
    "            y='t-SNE 2',\n",
    "            hue='classification',\n",
    "            palette=palette,\n",
    "            s=60,\n",
    "            ax=ax_tsne\n",
    "        )\n",
    "        ax_tsne.set_xlabel('t-SNE 1', fontsize=fontsize)\n",
    "        ax_tsne.set_ylabel('t-SNE 2', fontsize=fontsize)\n",
    "\n",
    "        # Remove legend\n",
    "        ax_tsne.legend_.remove()\n",
    "\n",
    "        # Formatting\n",
    "        sns.despine(ax=ax_tsne)\n",
    "        ax_tsne.spines['bottom'].set_linewidth(2)\n",
    "        ax_tsne.spines['left'].set_linewidth(2)\n",
    "        ax_tsne.spines['top'].set_visible(False)\n",
    "        ax_tsne.spines['right'].set_visible(False)\n",
    "        ax_tsne.xaxis.set_tick_params(length=10, width=2, labelsize=labelsize)\n",
    "        ax_tsne.yaxis.set_tick_params(length=10, width=2, labelsize=labelsize)\n",
    "\n",
    "    elif num_features == 2:\n",
    "        ax_scatter = axes[num_features] if num_features < len(axes) else fig.add_subplot(rows, 3, num_features + 1)\n",
    "\n",
    "        feature_x, feature_y = X_test.columns[0], X_test.columns[1]\n",
    "        x_label = feature_name_mapping.get(feature_x, feature_x)\n",
    "        y_label = feature_name_mapping.get(feature_y, feature_y)\n",
    "\n",
    "        sns.scatterplot(\n",
    "            data=df_errors,\n",
    "            x=feature_x,\n",
    "            y=feature_y,\n",
    "            hue='classification',\n",
    "            palette=palette,\n",
    "            s=60,\n",
    "            ax=ax_scatter\n",
    "        )\n",
    "\n",
    "        ax_scatter.set_xlabel(x_label, fontsize=fontsize)\n",
    "        ax_scatter.set_ylabel(y_label, fontsize=fontsize)\n",
    "\n",
    "        # Remove legend\n",
    "        ax_scatter.legend_.remove()\n",
    "\n",
    "        # Formatting\n",
    "        sns.despine(ax=ax_scatter)\n",
    "        ax_scatter.spines['bottom'].set_linewidth(2)\n",
    "        ax_scatter.spines['left'].set_linewidth(2)\n",
    "        ax_scatter.spines['top'].set_visible(False)\n",
    "        ax_scatter.spines['right'].set_visible(False)\n",
    "        ax_scatter.xaxis.set_tick_params(length=10, width=2, labelsize=labelsize)\n",
    "        ax_scatter.yaxis.set_tick_params(length=10, width=2, labelsize=labelsize)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(num_features + 1, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_summary(best_model, X_test, model=\"rf\", figsize=(9, 4)):\n",
    "    # Recovering the best model from the pipeline (if SMOTE/SMOTE-NC was used)\n",
    "    if hasattr(best_model, 'steps'):\n",
    "        best_model = best_model.steps[-1][1]\n",
    "\n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "    # Retrieve SHAP values and feature names\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    if model == \"rf\":\n",
    "        shap_values = shap_values[1]\n",
    "    elif model == \"xgb\":\n",
    "        shap_values = shap_values\n",
    "        \n",
    "    feature_names = X_test.columns\n",
    "    feature_values = X_test.values\n",
    "\n",
    "    # Map the feature names using the global feature_name_mapping\n",
    "    mapped_feature_names = [feature_name_mapping.get(name, name) for name in feature_names]\n",
    "\n",
    "    # Calculate the average magnitude of the SHAP values for each feature\n",
    "    feature_magnitudes = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Create a sorted order based on the magnitude of SHAP values\n",
    "    sorted_indices = np.argsort(feature_magnitudes)[::-1]\n",
    "    sorted_feature_names = [mapped_feature_names[i] for i in sorted_indices]\n",
    "\n",
    "    # Sort SHAP values and feature values accordingly\n",
    "    sorted_shap_values = shap_values[:, sorted_indices]\n",
    "    sorted_feature_values = feature_values[:, sorted_indices]\n",
    "\n",
    "    # Custom colormap for feature values\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#3C89F3\", \"#EA3355\"])\n",
    "\n",
    "    # Compute adaptive x-axis limits separately for positive and negative values\n",
    "    min_shap = np.min(shap_values)\n",
    "    max_shap = np.max(shap_values)\n",
    "\n",
    "    xlim_min = np.floor(min_shap / 0.05) * 0.05\n",
    "    xlim_max = np.ceil(max_shap / 0.05) * 0.05\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # For each feature, create a scatter plot\n",
    "    for i, feature_name in enumerate(sorted_feature_names):\n",
    "        # Retrieve the SHAP values and feature values for each feature\n",
    "        shap_vals = sorted_shap_values[:, i]\n",
    "        feature_vals = sorted_feature_values[:, i]\n",
    "\n",
    "        # Add jitter for spacing between points and reduce overlap\n",
    "        jitter = np.random.normal(0, 0.1, size=len(shap_vals))\n",
    "        y_pos = np.full(len(shap_vals), len(sorted_feature_names) - i + 0.5 * jitter)\n",
    "\n",
    "        # Scatter plot with color-coded feature values\n",
    "        plt.scatter(\n",
    "            shap_vals, \n",
    "            y_pos, \n",
    "            c=feature_vals, \n",
    "            cmap=cmap,\n",
    "            linewidth=0.5\n",
    "        )\n",
    "\n",
    "    # Configure labels and title\n",
    "    plt.yticks(range(1, len(sorted_feature_names) + 1), reversed(sorted_feature_names), fontsize=16)\n",
    "    plt.xlabel(\"SHAP value (impact on model output)\", fontsize=16)\n",
    "    plt.ylabel(\"\", fontsize=16)\n",
    "\n",
    "    # Apply custom x-axis limits with a small extension of Â± 0.02\n",
    "    plt.xlim(xlim_min - 0.02, xlim_max + 0.02)\n",
    "\n",
    "    # Apply ticks separately for negative and positive ranges\n",
    "    neg_ticks = np.arange(xlim_min, 0, 0.05)\n",
    "    pos_ticks = np.arange(0, xlim_max + 0.05, 0.05)\n",
    "    plt.xticks(np.concatenate([neg_ticks, pos_ticks]))\n",
    "\n",
    "    # Adjust the colorbar\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"Feature Value\", fontsize=16)\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_ticklabels([\"Low\", \"High\"], fontsize=14)\n",
    "\n",
    "    # Remove ticks on the y-axis but keep the feature names\n",
    "    plt.tick_params(axis='y', which='both', left=False)\n",
    "\n",
    "    # Increase x-axis line width and tick label size\n",
    "    plt.tick_params(axis='x', which='both', labelsize=16, width=2)\n",
    "    plt.gca().tick_params(axis='x', width=2, length=10)\n",
    "\n",
    "    # Remove top, left, and right borders\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    # Add a vertical line at x = 0\n",
    "    plt.axvline(x=0, color=\"black\", linestyle=\"-\", linewidth=2)\n",
    "\n",
    "    # Display the plot with tight layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_scatter(best_model, X_test, model=\"rf\", figsize=(14, 12)):\n",
    "    # Recovering the best model from the pipeline (if SMOTE/SMOTE-NC was used)\n",
    "    if hasattr(best_model, 'steps'):\n",
    "        best_model = best_model.steps[-1][1]\n",
    "\n",
    "    # SHAP explainer creation\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    if model == \"rf\":\n",
    "        shap_values = shap_values[1]\n",
    "    elif model == \"xgb\":\n",
    "        shap_values = shap_values\n",
    "    feature_names = X_test.columns\n",
    "    feature_values = X_test.values\n",
    "\n",
    "    # Apply mapping, otherwise keep original name\n",
    "    mapped_feature_names = [feature_name_mapping.get(name, name) for name in feature_names]\n",
    "\n",
    "    # Sort features by absolute average importance SHAP\n",
    "    feature_magnitudes = np.abs(shap_values).mean(axis=0)\n",
    "    sorted_indices = np.argsort(feature_magnitudes)[::-1]\n",
    "    sorted_feature_names = [mapped_feature_names[i] for i in sorted_indices]\n",
    "    sorted_shap_values = shap_values[:, sorted_indices]\n",
    "    sorted_feature_values = feature_values[:, sorted_indices]\n",
    "\n",
    "    # Compute global SHAP value limits for all features\n",
    "    min_shap_all = np.min(sorted_shap_values)\n",
    "    max_shap_all = np.max(sorted_shap_values)\n",
    "    \n",
    "    # Creation of subplots (3x3)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Configurations des axes par feature\n",
    "    feature_config = {\n",
    "        \"Gender\": {\"xticks\": [0, 1], \"xticklabels\": ['W', 'M'], \"xlim\": (-0.5, 1.5)},\n",
    "        \"Pleasantness\": {\"xlim\": (-5, 5), \"xticks\": np.arange(-5, 6, 2.5)},\n",
    "        \"Emotional strength\": {\"xlim\": (0, 5), \"xticks\": np.arange(0, 6, 1)},\n",
    "        \"Intensity\": {\"xlim\": (0, 10), \"xticks\": np.arange(0, 11, 2.5)},\n",
    "        \"Familiarity\": {\"xlim\": (0, 10), \"xticks\": np.arange(0, 11, 2.5)},\n",
    "        \"Perceptive distance\": {\"xlim\": (0, 12), \"xticks\": np.arange(0, 13, 2)},\n",
    "        \"Number of words\": {\"xlim\": (0, 12), \"xticks\": np.arange(0, 13, 1), \"xticklabels\": np.arange(0, 13, 1)},\n",
    "        \"Semantic distance\": {\"xlim\": (0, 1), \"xticks\": np.arange(0, 1.25, 0.25)},\n",
    "    }\n",
    "\n",
    "    # Plot graphs\n",
    "    for i, feature_name in enumerate(sorted_feature_names):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "\n",
    "        ax = axes[i]\n",
    "        shap_vals = sorted_shap_values[:, i]\n",
    "        feature_vals = sorted_feature_values[:, i]\n",
    "\n",
    "        # Scatter plot\n",
    "        ax.scatter(feature_vals, shap_vals, color='black', linewidth=0.5)\n",
    "        ax.axhline(y=0, color='grey', linestyle='--', linewidth=2)\n",
    "\n",
    "        # Labels\n",
    "        ax.set_xlabel(feature_name, fontsize=16)\n",
    "        ax.set_ylabel(\"SHAP value\", fontsize=16)\n",
    "\n",
    "        # Use global SHAP value limits for Y axis\n",
    "        y_lim_neg = np.floor(min_shap_all / 0.05) * 0.05 - 0.02\n",
    "        y_lim_pos = np.ceil(max_shap_all / 0.05) * 0.05 + 0.02\n",
    "        ax.set_ylim(y_lim_neg, y_lim_pos)\n",
    "\n",
    "        # X axis configuration according to feature\n",
    "        config = feature_config.get(feature_name, {})\n",
    "        if \"xlim\" in config:\n",
    "            # Check if the feature is \"Semantic distance\"\n",
    "            if feature_name == \"Semantic distance\":\n",
    "                ax.set_xlim(config[\"xlim\"][0] - 0.02, config[\"xlim\"][1] + 0.02)\n",
    "            else:\n",
    "                ax.set_xlim(config[\"xlim\"][0] - 0.2, config[\"xlim\"][1] + 0.2)\n",
    "        if \"xticks\" in config:\n",
    "            ax.set_xticks(config[\"xticks\"])\n",
    "        if \"xticklabels\" in config:\n",
    "            ax.set_xticklabels(config[\"xticklabels\"], fontsize=16)\n",
    "        \n",
    "        # For \"Number of words\", labels 1/2 tick\n",
    "        if feature_name == \"Number of words\":\n",
    "            labels = ax.get_xticklabels()\n",
    "            for j in range(1, len(labels), 2):  # Hide every other label\n",
    "                labels[j] = ''\n",
    "            ax.set_xticklabels(labels)\n",
    "\n",
    "        # Customize axes\n",
    "        ax.tick_params(axis='x', length=10, width=2, labelsize=16)\n",
    "        ax.tick_params(axis='y', length=10, width=2, labelsize=16)\n",
    "\n",
    "        # Remove unnecessary borders\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_linewidth(2)\n",
    "        ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    # Remove empty subplots if fewer than 9 features\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Display adjustment\n",
    "    plt.tight_layout(pad=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_waterfall(best_model, X_test, trial_index, predicted_class, model=\"rf\"):\n",
    "    # Recovering the best model from the pipeline (if SMOTE/SMOTE-NC was used)\n",
    "    if hasattr(best_model, 'steps'):\n",
    "        best_model = best_model.steps[-1][1]\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    if model == \"rf\":\n",
    "        shap_vals_trial = shap_values[predicted_class][trial_index]\n",
    "        base_value = explainer.expected_value[1]\n",
    "    elif model == \"xgb\":\n",
    "        shap_vals_trial = shap_values[trial_index]\n",
    "        base_value = explainer.expected_value\n",
    "    predicted_value = base_value + shap_vals_trial.sum()\n",
    "\n",
    "    # Map feature names\n",
    "    original_feature_names = list(X_test.columns)\n",
    "    mapped_feature_names = [feature_name_mapping.get(name, name) for name in original_feature_names]\n",
    "\n",
    "    # Sort SHAP values by absolute magnitude\n",
    "    sorted_indices = sorted(range(len(shap_vals_trial)), key=lambda i: abs(shap_vals_trial[i]))\n",
    "    shap_vals_trial_sorted = [shap_vals_trial[i] for i in sorted_indices]\n",
    "    custom_feature_names_sorted = [mapped_feature_names[i] for i in sorted_indices]\n",
    "\n",
    "    # Calculation of cumulative values of f(x)\n",
    "    cumulative_values = [base_value]\n",
    "    current_value = base_value\n",
    "\n",
    "    for shap_value in shap_vals_trial_sorted:\n",
    "        current_value += shap_value\n",
    "        cumulative_values.append(current_value)\n",
    "\n",
    "    # Adjustment of X-axis terminals according to values reached\n",
    "    x_min = min(cumulative_values) - 0.01\n",
    "    x_max = max(cumulative_values) + 0.01\n",
    "\n",
    "    # Plot settings\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    position = base_value\n",
    "    bar_height = 0.8\n",
    "    arrow_head_fraction = 0.1\n",
    "\n",
    "    arrow_end_positions = []\n",
    "    arrow_centers = []\n",
    "\n",
    "    for i, shap_value in enumerate(shap_vals_trial_sorted):\n",
    "        color = \"#EA3355\" if shap_value > 0 else \"#3C89F3\"\n",
    "        y_center = i\n",
    "        y_bottom = y_center - bar_height / 2\n",
    "        y_top = y_center + bar_height / 2\n",
    "        \n",
    "        x_start = position\n",
    "        x_end = position + shap_value\n",
    "        arrow_width = arrow_head_fraction * abs(shap_value)\n",
    "        rectangle_end = x_end - arrow_width if shap_value > 0 else x_end + arrow_width\n",
    "\n",
    "        arrow_vertices = [\n",
    "            (x_start, y_bottom), (rectangle_end, y_bottom),\n",
    "            (x_end, y_center), (rectangle_end, y_top), (x_start, y_top)\n",
    "        ]\n",
    "\n",
    "        arrow = Polygon(arrow_vertices, closed=True, facecolor=color, edgecolor='none')\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "        text_x = (x_start + rectangle_end) / 2\n",
    "        text_ha = \"center\"\n",
    "\n",
    "        if abs(shap_value) < 0.016:\n",
    "            text_x = x_end + (0.001 if shap_value > 0 else -0.001)\n",
    "            text_ha = \"left\" if shap_value > 0 else \"right\"\n",
    "\n",
    "        shap_text = f\"$\\\\bf{{{shap_value:+.2f}}}$\"\n",
    "\n",
    "        ax.text(\n",
    "            text_x, y_center, shap_text, ha=text_ha, va='center',\n",
    "            color=\"white\" if abs(shap_value) >= 0.016 else color, fontsize=12\n",
    "        )\n",
    "\n",
    "        position += shap_value\n",
    "        arrow_end_positions.append(x_end)\n",
    "        arrow_centers.append(y_center)\n",
    "\n",
    "    for i in range(len(arrow_centers) - 1):\n",
    "        ax.plot([arrow_end_positions[i], arrow_end_positions[i]], \n",
    "                [arrow_centers[i], arrow_centers[i + 1]], color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.axvline(predicted_value, color='grey', linestyle='--', linewidth=2)\n",
    "    ax.text(predicted_value, len(shap_vals_trial_sorted) + 0.2, \n",
    "            f\"$f(x) = {predicted_value:.3f}$\", ha='left', va='center', fontsize=16, color='black')\n",
    "\n",
    "    # Base_value limited to the height of features\n",
    "    ax.plot([base_value, base_value], [-0.5, 0.5],\n",
    "        color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.text(base_value, -2, f\"$E[f(X)] = {base_value:.3f}$\", ha='center', va='center', fontsize=16, color='black')\n",
    "\n",
    "    ax.set_yticks(range(len(shap_vals_trial_sorted)))\n",
    "    ax.set_yticklabels([\n",
    "        f\"{custom_feature_names_sorted[i]} = {X_test.iloc[trial_index, sorted_indices[i]]:.2f}\"\n",
    "        if custom_feature_names_sorted[i] in [\"Semantic distance\", \"Perceptive distance\"] \n",
    "        else f\"{custom_feature_names_sorted[i]} = {X_test.iloc[trial_index, sorted_indices[i]]}\"\n",
    "        for i in range(len(shap_vals_trial_sorted))\n",
    "    ], fontsize=16, color='black')\n",
    "\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "    ax.tick_params(axis='x', which='both', bottom=True, width=2, length=10, labelsize=16)\n",
    "    ax.tick_params(axis='y', which='both', length=0)\n",
    "    ax.set_ylim(-0.5, len(shap_vals_trial_sorted))\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Actual test value {trial_index}: {y_test.iloc[trial_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data loading & Preprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Loading](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory of the current working directory\n",
    "project_dir = Path.cwd().parent\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_folder = project_dir / \"data\"\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = data_folder / \"dataset.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display first lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Condition selection](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a condition\n",
    "condition = \"odor_recognition\"  # \"odor_recognition\" or \"associative_memory\"\n",
    "\n",
    "# Initialize variables according to condition\n",
    "if condition == \"odor_recognition\":\n",
    "    data = df[df[\"is_target\"] == 1].copy()\n",
    "    data[\"outcome\"] = data[\"hit\"]\n",
    "    data[\"percept_dist\"] = data[\"avg_distance_target\"]\n",
    "    data[\"jaccard_dist\"] = data[\"mean_lemma_jaccard_target\"]\n",
    "    fillcolor = \"#5187EC\"\n",
    "    final_features = [\"gender_encoded\", \"pleasantness\", \"emotional_strength\", \"intensity\", \"familiarity\", \"nb_words\", \"jaccard_dist\"]\n",
    "    palette = {\n",
    "        'Correct Class 0': '#F39C12', \n",
    "        'Misclassified 1->0': '#C75F1D',\n",
    "        'Correct Class 1': '#3C89F3',\n",
    "        'Misclassified 0->1': '#1F4E79'\n",
    "    }\n",
    "\n",
    "elif condition == \"associative_memory\":\n",
    "    data = df[df[\"hit\"] == 1].copy()\n",
    "    data[\"outcome\"] = data[\"mem\"]\n",
    "    data[\"percept_dist\"] = data[\"avg_distance_hit\"]\n",
    "    data[\"jaccard_dist\"] = data[\"mean_lemma_jaccard_hit\"]\n",
    "    fillcolor = \"#5EACA3\"\n",
    "    first_feature_sel = [\"emotional_strength\", \"familiarity\", \"percept_dist\", \"nb_words\", \"jaccard_dist\"]\n",
    "    second_feature_sel = [\"familiarity\", \"percept_dist\", \"nb_words\", \"jaccard_dist\"]\n",
    "    final_features = [\"familiarity\", \"jaccard_dist\"]\n",
    "    palette = {\n",
    "        'Correct Class 0': '#F2645A', \n",
    "        'Misclassified 1->0': '#A73C35',\n",
    "        'Correct Class 1': '#3CAEA3',\n",
    "        'Misclassified 0->1': '#0F524B'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Info on data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_1_'></a>[Check for unique values (n of sub, n of odor, ...)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_2_'></a>[Target distribution](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(data[\"outcome\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[Features/Target split](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate numerical and categorical features\n",
    "features = [\n",
    "    'gender_encoded',\n",
    "    'pleasantness',\n",
    "    'emotional_strength',\n",
    "    'intensity',\n",
    "    'familiarity', \n",
    "    'percept_dist',\n",
    "    'nb_words',\n",
    "    'jaccard_dist',\n",
    "    ]\n",
    "\n",
    "X = data.loc[:, features]\n",
    "y = data[\"outcome\"]\n",
    "\n",
    "groups = data[\"participant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Train/Test split](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data includes multiple rows for each participant. As our aim is to make predictions for new participants, then testing on rows from participants who also have rows in the training set may be optimistically biased. After all, the responses of the same participant are bound to be more similar than those of two different participants. Using StratifiedGroupKFold ensures that participants remain grouped and that the outcome distribution is maintained in each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome repartition in y\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedGroupKFold initialization\n",
    "splitter = StratifiedGroupKFold(n_splits = 5)\n",
    "\n",
    "# Obtaining training and test set indices\n",
    "train_indx, test_indx = next(splitter.split(X, y, groups))\n",
    "\n",
    "# Length of train and test\n",
    "print(f\"Train length: n={len(train_indx)}\")\n",
    "print(f\"Test length: n={len(test_indx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X into train & test\n",
    "X_train = X.iloc[train_indx]\n",
    "X_test = X.iloc[test_indx]\n",
    "\n",
    "# Split y into train & test\n",
    "y_train = y.iloc[train_indx]\n",
    "y_test = y.iloc[test_indx]\n",
    "\n",
    "# Split groups into train & test\n",
    "groups_train = groups.iloc[train_indx]\n",
    "groups_test = groups.iloc[test_indx]\n",
    "\n",
    "# Outcome repartition in y_train\n",
    "print(f\"Outcome repartition in y_train : {y_train.value_counts(normalize=True)}\")\n",
    "print()\n",
    "\n",
    "# Outcome repartition in y_test\n",
    "print(f\"Outcome repartition in y_test : {y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participants ID in the training set\n",
    "unique_groups_train = groups_train.unique()\n",
    "print(f\"Participants in groups_train: {unique_groups_train}\")\n",
    "print()\n",
    "\n",
    "# Participants ID in the test set\n",
    "unique_groups_test = groups_test.unique()\n",
    "print(f\"Participants in groups_test: {unique_groups_test}\")\n",
    "print()\n",
    "\n",
    "# Verif\n",
    "common_groups = set(unique_groups_train).intersection(unique_groups_test)\n",
    "\n",
    "if common_groups:\n",
    "    print(f\"Participants in both groups: {common_groups}\")\n",
    "else:\n",
    "    print(\"There is no common participants in groups_train and groups_test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_6_'></a>[Data Shuffling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use shuffle to randomly shuffle the data to ensure that the order of the samples does not introduce unwanted bias into the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sh, y_train_sh, groups_train_sh = shuffle(X_train, y_train, groups_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index of training and test sets\n",
    "X_train_ri, X_test_ri = X_train_sh.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "y_train_ri, y_test_ri = y_train_sh.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "groups_train_ri = groups_train_sh.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Features selection](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the features\n",
    "X_train_ri_all = X_train_ri.copy()\n",
    "X_test_ri_all = X_test_ri.copy()\n",
    "\n",
    "# Final features\n",
    "X_train_ri_final = X_train_ri[final_features]\n",
    "X_test_ri_final = X_test_ri[final_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Base model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "base_model, results_bm = perform_random_forest(\n",
    "    X_train_ri_all,\n",
    "    y_train_ri,\n",
    "    groups_train_ri,\n",
    "    model_type=\"default_rf\",\n",
    "    tuning=None,\n",
    "    categorical_features=\"Yes\"\n",
    "    )\n",
    "\n",
    "# Permutation test\n",
    "permutation_test_with_plot(\n",
    "    model=base_model,\n",
    "    X=X_train_ri_all,\n",
    "    y=y_train_ri,\n",
    "    groups=groups_train_ri\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(base_model, X_test_ri_all, y_test_ri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[All features - models with tuning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "first_brf, results_first_brf = perform_random_forest(\n",
    "    X_train_ri_all,\n",
    "    y_train_ri,\n",
    "    groups_train_ri,\n",
    "    model_type=\"balanced_rf\",\n",
    "    tuning=True,\n",
    "    categorical_features=\"Yes\"\n",
    "    )\n",
    "\n",
    "# Permutation test\n",
    "permutation_test_with_plot(\n",
    "    model=first_brf,\n",
    "    X=X_train_ri_all,\n",
    "    y=y_train_ri,\n",
    "    groups=groups_train_ri\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(first_brf, X_test_ri_all, y_test_ri)\n",
    "\n",
    "# Feature selection\n",
    "shap_feature_selection(first_brf, X_train_ri_all, y_train_ri, model=\"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if condition == \"odor_recognition\":\n",
    "    model_type = \"smote_rf\"\n",
    "elif condition == \"associative_memory\":\n",
    "    model_type = \"default_rf\"\n",
    "\n",
    "# Training\n",
    "first_smrf, results_first_smrf = perform_random_forest(\n",
    "    X_train_ri_all,\n",
    "    y_train_ri,\n",
    "    groups_train_ri,\n",
    "    model_type=model_type,\n",
    "    tuning=True,\n",
    "    categorical_features=\"Yes\"\n",
    "    )\n",
    "\n",
    "# Permutation test\n",
    "permutation_test_with_plot(\n",
    "    model=first_smrf,\n",
    "    X=X_train_ri_all,\n",
    "    y=y_train_ri,\n",
    "    groups=groups_train_ri\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(first_smrf, X_test_ri_all, y_test_ri)\n",
    "\n",
    "# Feature selection\n",
    "shap_feature_selection(first_smrf, X_train_ri_all, y_train_ri, model=\"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "first_xgb, results_first_xgb = perform_xgboost(\n",
    "    X_train_ri_all,\n",
    "    y_train_ri,\n",
    "    groups_train_ri\n",
    "    )\n",
    "\n",
    "# Permutation test\n",
    "permutation_test_with_plot(\n",
    "    model=first_xgb,\n",
    "    X=X_train_ri_all,\n",
    "    y=y_train_ri,\n",
    "    groups=groups_train_ri\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(first_xgb, X_test_ri_all, y_test_ri)\n",
    "\n",
    "# Feature selection\n",
    "shap_feature_selection(first_xgb, X_train_ri_all, y_train_ri, model=\"xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Final models](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "final_brf, results_final_brf = perform_random_forest(\n",
    "    X_train_ri_final,\n",
    "    y_train_ri,\n",
    "    groups_train_ri,\n",
    "    model_type=\"balanced_rf\",\n",
    "    tuning=True,\n",
    "    categorical_features=\"Yes\"\n",
    "    )\n",
    "\n",
    "# Permutation test\n",
    "permutation_test_with_plot(\n",
    "    model=final_brf,\n",
    "    X=X_train_ri_final,\n",
    "    y=y_train_ri,\n",
    "    groups=groups_train_ri\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(final_brf, X_test_ri_final, y_test_ri)\n",
    "\n",
    "# Error analysis\n",
    "error_analysis(final_brf, X_test_ri_final, y_test_ri)\n",
    "\n",
    "# Interpretability\n",
    "plot_shap_summary(final_brf, X_test_ri_final, model=\"rf\" , figsize=(9, 4))\n",
    "\n",
    "plot_shap_scatter(final_brf, X_test_ri_final, model=\"rf\")\n",
    "\n",
    "plot_shap_waterfall(final_brf, X_test_ri_final, trial_index=70, predicted_class=1, model=\"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "final_smrf, results_final_smrf = perform_random_forest(\n",
    "    X_train_ri_final,\n",
    "    y_train_ri,\n",
    "    groups_train_ri,\n",
    "    model_type=model_type,\n",
    "    tuning=True,\n",
    "    categorical_features=\"Yes\"\n",
    "    )\n",
    "\n",
    "# Permutation test\n",
    "permutation_test_with_plot(\n",
    "    model=final_smrf,\n",
    "    X=X_train_ri_final,\n",
    "    y=y_train_ri,\n",
    "    groups=groups_train_ri\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(final_smrf, X_test_ri_final, y_test_ri)\n",
    "\n",
    "# Error analysis\n",
    "error_analysis(final_smrf, X_test_ri_final, y_test_ri)\n",
    "\n",
    "# Interpretability\n",
    "plot_shap_summary(final_smrf, X_test_ri_final, model=\"rf\", figsize=(9, 4))\n",
    "\n",
    "plot_shap_scatter(final_smrf, X_test_ri_final, model=\"rf\")\n",
    "\n",
    "plot_shap_waterfall(final_smrf, X_test_ri_final, trial_index=70, predicted_class=1, model=\"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "final_xgb, results_final_xgb = perform_xgboost(\n",
    "    X_train_ri_final,\n",
    "    y_train_ri,\n",
    "    groups_train_ri\n",
    "    )\n",
    "\n",
    "# Permutation test\n",
    "permutation_test_with_plot(\n",
    "    model=final_xgb,\n",
    "    X=X_train_ri_final,\n",
    "    y=y_train_ri,\n",
    "    groups=groups_train_ri\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(final_xgb, X_test_ri_final, y_test_ri)\n",
    "\n",
    "# Error analysis\n",
    "error_analysis(final_xgb, X_test_ri_final, y_test_ri)\n",
    "\n",
    "# Interpretability\n",
    "plot_shap_summary(final_xgb, X_test_ri_final, model=\"xgb\", figsize=(9, 4))\n",
    "\n",
    "plot_shap_scatter(final_xgb, X_test_ri_final, model=\"xgb\")\n",
    "\n",
    "plot_shap_waterfall(final_xgb, X_test_ri_final, trial_index=70, predicted_class=1, model=\"xgb\")\n",
    "\n",
    "# nb: XGBoost gives a log-odds score, not direct probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_oldmem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
